import os, sys
import h5py 
import rdkit.Chem as Chem 
import rdkit.Chem.AllChem as AllChem
import random 
import numpy as np
from multiprocessing import Pool

'''
This script is used to generate an .h5 file containing all the fingerprints 
needed to train the model, so they do not have to be generated on the fly.

It needs to be called with a command-line argument containing the path to the 
.txt file; in our case, this is the txt file generated by get_reaxys_data.py

This script uses a pool of workers to generate fingerprints to speed the 
process up a little.
'''

path = sys.argv[1]

if not os.path.isfile(path):
	quit('Need to specify a file to read from')

data = []
with open(path, 'r') as f:
    for line in f:
        rex, n, _id = line.strip("\r\n").split(' ')
        r,p = rex.split('>>')
        if ('.' in p) or (not p):
            continue # do not allow multiple products or none
        n = int(n)
        for r_splt in r.split('.'):
            if r_splt:
                data.append((_id, n, r_splt, p))
random.seed(123) # always use this seed to get the same split, arbitrarly chosen
random.shuffle(data)
print('Read and shuffled {} total data after splitting'.format(len(data)))

FP_len = 1024
FP_rad = 2
def mol_to_fp(mol, radius=FP_rad, nBits=FP_len):
    if mol is None:
        return np.zeros((nBits,), dtype=np.bool)
    return np.array(AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=nBits, useChirality=True), dtype=np.bool)

def smi_to_fp(smi, radius=FP_rad, nBits=FP_len):
    if not smi:
        return np.zeros((nBits,), dtype=np.bool)
    return mol_to_fp(Chem.MolFromSmiles(smi), radius, nBits)

def chunks(l, n):
    """Yield successive n-sized chunks from l."""
    for i in range(0, len(l), n):
        yield l[i:i + n]

if os.path.isfile(path + '.h5'):
	f = h5py.File(path + '.h5', 'r+')
	dset = f['data_fps']
else:
	f = h5py.File(path + '.h5', 'w')
	dset = f.create_dataset('data_fps', (len(data)*2, FP_len), dtype=np.bool)

done = 0
try:

	pool = Pool(8)
	ALREADY_DONE = 0
	for lst in chunks(range(ALREADY_DONE, len(data)), 50000): # 50k chunks
		for i, fp in enumerate(pool.imap(smi_to_fp, [data[i][2] for i in lst], chunksize=500)):
			dset[2*lst[i], :] = fp 
		for i, fp in enumerate(pool.imap(smi_to_fp, [data[i][3] for i in lst], chunksize=500)):
			dset[2*lst[i]+1, :] = fp

		print('latest index done: %i/%i' % (lst[-1]+1, len(data)))
		done += len(lst)
		if done >= 250000:
			f.close()
			f = h5py.File(path + '.h5', 'r+')
			dset = f['data_fps']
			done = 0
			print('Closed and reopened file!')

finally:
	f.close()
	pool.close()
	pool.join()